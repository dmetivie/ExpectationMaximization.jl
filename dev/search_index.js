var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I plan to add benchmarks similar to here. However, I am not too sure how to do 100% fair comparisons across languages.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Obviously I would test only Gaussian Mixture since it is the most common type of mixture (remembering that this packages allow plenty of mixtures).","category":"page"},{"location":"fit_mle/#InstanceVType","page":"Instance vs Type version","title":"Instance vs Type version","text":"","category":"section"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"This package relies heavily on the types and methods defined in Distributions.jl e.g. fit_mle and logpdf. However, it differs slightly by one point: it defines and uses an \"instance version\" of fit_mle(dist::Distribution, x, args...). For the package to work and be generic, the input dist must be an \"instance\" i.e. an actual distribution like Normal(0,1) not just the type Normal. For MixtureModels (and ProductDistributions) this is critical to extract the various distributions inside while the Type version does not (currently) contain such information. Plus, for MixtureModels the dist serves as the initial point of EM algorithm. For classic distributions, it changes nothing i.e. fit_mle(Normal(0,1), y) gives the same result as fit_mle(Normal, y).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I opened a PR#1670 in Distributions.jl to include this instance version, but it might not be accepted.","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"note: Compatibility\nThe new fit_mle methods defined in this package are fully compatible with Distributions.jl (it does not break any regular Distributions.fit_mle).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I believe that some packages dealing with complex distributions like Copulas.jl or HMMBase.jl could also use this formulation.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributions\nusing ExpectationMaximization\nusing StatsPlots","category":"page"},{"location":"examples/#Univariate-continuous","page":"Examples","title":"Univariate continuous","text":"","category":"section"},{"location":"examples/#Exponential-Laplace-Exponential","page":"Examples","title":"Exponential + Laplace + Exponential","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Parameters\nN = 5_000\nθ₁ = 5\nθ₂ = 0.8\nα = 0.5\nβ = 0.3\nμ = -1\na = 2\n\nmix_true = MixtureModel([Laplace(μ, θ₁), Normal(α, θ₂), Exponential(a)], [β/2, 1 - β, β/2])\n\n# Components of the mixtures\nplot(mix_true, label = [\"Laplace\" \"Normal\" \"Exponential\"])\nylabel!(\"Log PDF\", yaxis = :log10)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Sampling\ny = rand(mix_true, N)\n\n# Initial Condition\nmix_guess = MixtureModel([Laplace(-1, 1), Normal(2, 1), Exponential(3)], [1/3, 1/3, 1/3])\n\n# Fit Classic EM\nmix_mle_C, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = ClassicEM())\n\n# Fit Stochastic EM\nmix_mle_S, hist_S = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = StochasticEM())\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle_C, x), label = \"fit EM\")\nplot!(pmix, x, pdf.(mix_mle_S, x), label = \"fit sEM\")\n\nploss = plot(hist_C[\"logtots\"], label = \"ClassicEM with $(hist_C[\"iterations\"]) iterations\", c = 3, xlabel = \"EM iterations\", ylabel = \"Log Likelihood\")\nplot!(ploss, hist_S[\"logtots\"], label = \"StochasticEM  with $(hist_S[\"iterations\"]) iterations\", c = 4, s = :dot)\n\nplot(pmix, ploss)","category":"page"},{"location":"examples/#Mixture-of-Mixture-and-univariate","page":"Examples","title":"Mixture of Mixture and univariate","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = -5\nθ₂ = 2\nσ₁ = 1\nσ₂ = 1.5\nθ₀ = 0.1\nσ₀ = 0.1\n\nα = 1 / 2\nβ = 0.3\n\nd1 = MixtureModel([Laplace(θ₁, σ₁), Normal(θ₂, σ₂)], [α, 1 - α])\nd2 = Normal(θ₀, σ₀)\nmix_true = MixtureModel([d1, d2], [β, 1 - β])\ny = rand(mix_true, N)\n\n# We choose initial guess very close to the true solution just to show the EM algorithm convergence.\n# This particular choice of mixture of mixture Gaussian with another Gaussian is non identifiable hence we execpt other solution far away from the true solution\nd1_guess = MixtureModel([Laplace(θ₁ - 2, σ₁ + 1), Normal(θ₂ + 1, σ₂ + 1)], [α + 0.1, 1 - α - 0.1])\nd2_guess = Normal(θ₀ - 1, σ₀ - 0.05)\n\nmix_guess = MixtureModel([d1_guess, d2_guess], [β + 0.1, 1 - β - 0.1])\nmix_mle, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true)\ny_guess = rand(mix_mle, N)\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle, x), label = \"fit EM with $(hist_C[\"iterations\"]) iterations\")","category":"page"},{"location":"examples/#Multivariate-mixtures","page":"Examples","title":"Multivariate mixtures","text":"","category":"section"},{"location":"examples/#Multivariate-Gaussian-Mixtures","page":"Examples","title":"Multivariate Gaussian Mixtures","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = [-1, 1]\nθ₂ = [0, 2]\nΣ₁ = [0.5 0.5\n    0.5 1]\nΣ₂ = [1 0.1\n    0.1 1]\nβ = 0.3\nD₁ = MvNormal(θ₁, Σ₁)\nD₂ = MvNormal(θ₂, Σ₂)\n\nmix_true = MixtureModel([D₁, D₂], [β, 1 - β])\n\n# Generate samples from the true distribution\ny = rand(mix_true, N)\n\n# Initial Condition\nD₁guess = MvNormal([0.2, 1], [1 0.6; 0.6 1])\nD₂guess = MvNormal([1, 0.5], [1 0.2; 0.2 1])\nmix_guess = MixtureModel([D₁guess, D₂guess], [0.4, 0.6])\n\n# Fit MLE\nmix_mle = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"examples/#Bernoulli-Mixture","page":"Examples","title":"Bernoulli Mixture","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"S = 10\nK = 3\nθ = zeros(S, K)\nθ[:, 1] = (1:S) / S .- 0.05 # Bernoulli parameters\nθ[:, 2] = (S:-1:1) / 2S # Bernoulli parameters\nθ[:, 3] = ones(S) + 0.1 * [isodd(i) ? -1 : 1 for i in 1:S] .- 0.4# Bernoulli parameters\nβ = 0.3\n\nmix_true = MixtureModel([product_distribution(Bernoulli.(θ[:, i])) for i in 1:K], [β / 2, 1 - β, β / 2])\n\n# Generate samples from the true distribution\ny = rand(mix_true, N)\n\n# Initial Condition\nmix_guess = MixtureModel([product_distribution(Bernoulli.(2θ[:, i] / 3)) for i in 1:K], [0.25, 0.55, 0.2])\n\n# Fit MLE\nmix_mle = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"#ExpectationMaximization.jl","page":"Home","title":"ExpectationMaximization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a simple implementation of the Expectation Maximization (EM) algorithm used to fit mixture models. Due to Julia amazing dispatch systems, generic and reusable code spirit, and the Distributions.jl package, the code while being very generic is also very powerful! That means that it work on a lot of mixture:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Mixture of Univariate continuous distributions\nMixture of Univariate discrete distributions\nMixture of Multivariate distributions (continuous or discrete).\nMixture of mixtures (univariate or multivariate and continuous or discrete).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that Distributions currently does not allow MixtureModel to both have discrete and continuous components (but who does that? Rain).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Have a look at the examples section.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To work, the only requirements are that the dist<:Distribution considered has","category":"page"},{"location":"","page":"Home","title":"Home","text":"logpdf(dist, y) (used in the E-step)\nfit_mle(dist, y, weigths) (used in the M-step)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general, 1. is easy, while 2. is only known explicitly for a few common distributions. In case 2. is not explicit known, you can always implement a numerical scheme, if it exists, for fit_mle(dist, y) see Gamma distribution example. Or, when possible, represent your “difficult” distribution as a mixture of simple terms. (I had this in mind, but it is not directly a mixture model.)","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ClassicEM","category":"page"},{"location":"#ExpectationMaximization.ClassicEM","page":"Home","title":"ExpectationMaximization.ClassicEM","text":"ClassicEM<:AbstractEM\n\nThe EM algorithm was introduced by A. P. Dempster, N. M. Laird and D. B. Rubin in 1977 in the reference paper Maximum Likelihood from Incomplete Data Via the EM Algorithm.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"StochasticEM","category":"page"},{"location":"#ExpectationMaximization.StochasticEM","page":"Home","title":"ExpectationMaximization.StochasticEM","text":"Base.@kwdef struct StochasticEM<:AbstractEM \n    rng::AbstractRNG = Random.GLOBAL_RNG\nend\n\nThe Stochastic EM algorithm was introduced by G. Celeux, and J. Diebolt. in 1985 in The SEM Algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem.\n\nThe default random seed is Random.GLOBAL_RNG but it can be changed via StochasticEM(seed).\n\n\n\n\n\n","category":"type"},{"location":"#Main-function","page":"Home","title":"Main function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"warning: Warning\nUse the \"instance\" version of fit_mle(mix::MixtureModel, ...) as described bellow and NOT the \"Type\" version i.e. fit_mle(Type{MixtureModel}, ...). The provided mix is used as the starting point of the EM algorithm. See Instance vs Type version section for more context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fit_mle(mix::MixtureModel, y::AbstractVecOrMat, weights...; kwargs...)\nfit_mle(mix::AbstractArray{<:MixtureModel}, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"#Distributions.fit_mle-Tuple{MixtureModel, AbstractVecOrMat, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(mix::MixtureModel, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)\n\nUse the an Expectation Maximization (EM) algorithm to maximize the Loglikelihood (fit) the mixture with an i.i.d sample y. The mix input is a mixture that is used to initilize the EM algorithm.\n\nweights when provided will computed a weighted version of the EM. (Useful for fitting mixture of mixtures)\nmethod determines the algorithm used.\ninfos = true returns a Dict with informations on the algorithm.\nrobust = true will prevent the (log)likelihood to overflow to -∞ or ∞.\natol criteria determining the convergence of the algorithm. If the Loglikelihood difference between two iteration i and i+1 is smaller than atol i.e. |ℓ⁽ⁱ⁺¹⁾ - ℓ⁽ⁱ⁾|<atol, the algorithm stops. \ndisplay value can be :none, :iter, :final to display Loglikelihood evolution at each iterations :iter or just the final one :final\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.fit_mle-Tuple{AbstractArray{<:MixtureModel}, AbstractVecOrMat, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(mix::AbstractArray{<:MixtureModel}, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)\n\nDo the same as fit_mle for each (initial) mixtures in the mix array. Then it selects the one with the largest loglikelihood. Warning: It uses try and catch to avoid errors messages in case EM converges toward a singular solution (probably using robust should be enough in most case to avoid errors). \n\n\n\n\n\n","category":"method"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"most_likely_cat\nlikelihood_per_cat","category":"page"},{"location":"#ExpectationMaximization.most_likely_cat","page":"Home","title":"ExpectationMaximization.most_likely_cat","text":"most_likely_cat(mix::MixtureModel, y::AbstractVector; robust=false)\n\nEvaluate the most likely category of each observations.\n\nrobust = true will prevent the (log)likelihood to overflow to -∞ or ∞.\n\n\n\n\n\n","category":"function"},{"location":"#ExpectationMaximization.likelihood_per_cat","page":"Home","title":"ExpectationMaximization.likelihood_per_cat","text":"likelihood_per_cat(mix::MixtureModel, y::AbstractVecOrMat; robust=false)\n\nEvaluate the the probability for each observations to belong to a category.\n\nrobust = true will prevent the (log)likelihood to under(overflow)flow to -∞ (or ∞).\n\n\n\n\n\n","category":"function"},{"location":"#fit_mle-methods-that-should-be-in-Distribution.jl","page":"Home","title":"fit_mle methods that should be in Distribution.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"I opened two PRs PR#1670 and PR#1676 to add these methods.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fit_mle(g::Product, x::AbstractMatrix, args...)","category":"page"},{"location":"#Distributions.fit_mle-Tuple{Product, AbstractMatrix, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(g::Product, x::AbstractMatrix)\nfit_mle(g::Product, x::AbstractMatrix, γ::AbstractVector)\n\nThe fit_mle for multivariate Product distributions g is the product_distribution of fit_mle of each components of g. Product is meant to be depreacated in next version of Distribution.jl. Use the analog VectorOfUnivariateDistribution type instead.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"fit_mle(::Type{<:Dirac}, x::AbstractArray{T}, w::AbstractArray{Float64}) where {T<:Real}\nfit_mle(::Type{<:Laplace}, x::AbstractArray{<:Real}, w::AbstractArray{<:Real})","category":"page"},{"location":"#Distributions.fit_mle-Union{Tuple{T}, Tuple{Type{<:Dirac}, AbstractArray{T}, AbstractArray{Float64}}} where T<:Real","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(::Type{<:Dirac}, x::AbstractArray{<:Real}[, w::AbstractArray{<:Real}])\n\nfit_mle for Dirac distribution (weighted or not) data sets.\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.fit_mle-Tuple{Type{<:Laplace}, AbstractArray{<:Real}, AbstractArray{<:Real}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(::Type{<:Laplace}, x::AbstractArray{<:Real}, w::AbstractArray{<:Real})\n\nfit_mle for Laplace distribution weighted data sets.\n\n\n\n\n\n","category":"method"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"biblio/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"biblio/#Theory","page":"Bibliography","title":"Theory","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"The EM algorithm was introduced by A. P. Dempster, N. M. Laird and D. B. Rubin in 1977 in the reference paper Maximum Likelihood from Incomplete Data Via the EM Algorithm. This is a very generic algorithm working for almost any distributions. I also added the stochastic version introduced by G. Celeux, and J. Diebolt. in 1985 in The SEM Algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem. Other versions can be added PR are welcomed.","category":"page"},{"location":"biblio/#Implementations","page":"Bibliography","title":"Implementations","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Despite being generic, to my knowledge almost all coding implementations are specific to some mixtures class (mostly Gaussian mixtures, sometime double exponential or Bernoulli mixtures).","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In this package thanks to Julia generic code spirit one can just code the algorithm, and it works for all distributions.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I know of the Python mixem package doing also using a generic algorithm implementation. However, the available distributions choice is very limited as the authors have to define each distribution (Top-Down approach). This package does not define distribution[1], it simply uses the Distribution type and what is in Distributions.jl.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In Julia there is the GaussianMixtures.jl package that also do EM. It seems a little bit faster than my implementation when used with Gaussian mixtures (I'd like to understand what is creating this difference though). However, I am not sure if this is maintained anymore.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Have a look at the benchmark section for some comparisons.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I was inspired by Florian Oswald page and Maxime Mouchet HMMBase.jl package.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"[1]: I added fit_mle methods for Product distributions, weighted Laplace and Dirac. I am doing PR to merge that directly into the Distributions.jl package.","category":"page"}]
}
