var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I plan to add benchmarks similar to here. However, I am not too sure how to do 100% fair comparisons across languages.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Obviously I would test only Gaussian Mixture since it is the most common type of mixture (remembering that this packages allow plenty of mixtures).","category":"page"},{"location":"fit_mle/#InstanceVType","page":"Instance vs Type version","title":"Instance vs Type version","text":"","category":"section"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"This package relies heavily on the types and methods defined in Distributions.jl e.g. fit_mle and logpdf. However, it differs slightly by one point: it defines and uses an \"instance version\" of fit_mle(dist::Distribution, x, args...). For the package to work and be generic, the input dist must be an \"instance\" i.e. an actual distribution like Normal(0,1) not just the type Normal. For MixtureModels (and ProductDistributions) this is critical to extract the various distributions inside while the Type version does not (currently) contain such information. Plus, for MixtureModels the dist serves as the initial point of EM algorithm. For classic distributions, it changes nothing i.e. fit_mle(Normal(0,1), y) gives the same result as fit_mle(Normal, y).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I opened a PR#1670 in Distributions.jl to include this instance version, but it might not be accepted.","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"note: Compatibility\nThe new fit_mle methods defined in this package are fully compatible with Distributions.jl (it does not break any regular Distributions.fit_mle).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I believe that some packages dealing with complex distributions like Copulas.jl or HMMBase.jl could also use this formulation.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributions\nusing ExpectationMaximization\nusing StatsPlots","category":"page"},{"location":"examples/#Univariate-continuous","page":"Examples","title":"Univariate continuous","text":"","category":"section"},{"location":"examples/#Exponential-Laplace-Uniform","page":"Examples","title":"Exponential + Laplace + Uniform","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Parameters\nN = 5_000\nθ₁ = 5\nθ₂ = 0.8\nα = 0.5\nβ = 0.3\nμ = -1\na = 2\n\nmix_true = MixtureModel([Laplace(μ, θ₁), Normal(α, θ₂), Exponential(a)], [β/2, 1 - β, β/2])\n\n# Components of the mixtures\nplot(mix_true, label = [\"Laplace\" \"Normal\" \"Exponential\"])\nylabel!(\"Log PDF\", yaxis = :log10)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Sampling\ny = rand(mix_true, N)\n\n# Initial Condition\nmix_guess = MixtureModel([Laplace(-1, 1), Normal(2, 1), Exponential(3)], [1/3, 1/3, 1/3])\n\n# Fit Classic EM\nmix_mle_C, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = ClassicEM())\n\n# Fit Stochastic EM\nmix_mle_S, hist_S = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = StochasticEM())\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle_C, x), label = \"fit EM\")\nplot!(pmix, x, pdf.(mix_mle_S, x), label = \"fit sEM\")\n\nploss = plot(hist_C[\"logtots\"], label = \"ClassicEM with $(hist_C[\"iterations\"]) iterations\", c = 3, xlabel = \"EM iterations\", ylabel = \"Log Likelihood\")\nplot!(ploss, hist_S[\"logtots\"], label = \"StochasticEM  with $(hist_S[\"iterations\"]) iterations\", c = 4, s = :dot)\n\nplot(pmix, ploss)","category":"page"},{"location":"examples/#Mixture-of-Mixture-and-univariate","page":"Examples","title":"Mixture of Mixture and univariate","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = -5\nθ₂ = 2\nσ₁ = 1\nσ₂ = 1.5\nθ₀ = 0.1\nσ₀ = 0.1\n\nα = 1 / 2\nβ = 0.3\n\nd1 = MixtureModel([Laplace(θ₁, σ₁), Normal(θ₂, σ₂)], [α, 1 - α])\nd2 = Normal(θ₀, σ₀)\nmix_true = MixtureModel([d1, d2], [β, 1 - β])\ny = rand(mix_true, N)\n\n# We choose initial guess very close to the true solution just to show the EM algorithm convergence.\n# This particular choice of mixture of mixture Gaussian with another Gaussian is non identifiable hence we execpt other solution far away from the true solution\nd1_guess = MixtureModel([Laplace(θ₁ - 2, σ₁ + 1), Normal(θ₂ + 1, σ₂ + 1)], [α + 0.1, 1 - α - 0.1])\nd2_guess = Normal(θ₀ - 1, σ₀ - 0.05)\n\nmix_guess = MixtureModel([d1_guess, d2_guess], [β + 0.1, 1 - β - 0.1])\nmix_mle, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true)\ny_guess = rand(mix_mle, N)\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle, x), label = \"fit EM with $(hist_C[\"iterations\"]) iterations\")","category":"page"},{"location":"examples/#Multivariate-mixtures","page":"Examples","title":"Multivariate mixtures","text":"","category":"section"},{"location":"examples/#Multivariate-Gaussian-Mixtures","page":"Examples","title":"Multivariate Gaussian Mixtures","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = [-1, 1]\nθ₂ = [0, 2]\nΣ₁ = [0.5 0.5\n    0.5 1]\nΣ₂ = [1 0.1\n    0.1 1]\nβ = 0.3\nD₁ = MvNormal(θ₁, Σ₁)\nD₂ = MvNormal(θ₂, Σ₂)\n\nmix_true = MixtureModel([D₁, D₂], [β, 1 - β])\n\n# Generate samples from the true distribution\ny = rand(mix_true, N)\n\n# Initial Condition\nD₁guess = MvNormal([0.2, 1], [1 0.6; 0.6 1])\nD₂guess = MvNormal([1, 0.5], [1 0.2; 0.2 1])\nmix_guess = MixtureModel([D₁guess, D₂guess], [0.4, 0.6])\n\n# Fit MLE\nmix_mle = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"examples/#Bernoulli-Mixture","page":"Examples","title":"Bernoulli Mixture","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"S = 10\nK = 3\nθ = zeros(S, K)\nθ[:, 1] = (1:S) / S .- 0.05 # Bernoulli parameters\nθ[:, 2] = (S:-1:1) / 2S # Bernoulli parameters\nθ[:, 3] = ones(S) + 0.1 * [isodd(i) ? -1 : 1 for i in 1:S] .- 0.4# Bernoulli parameters\nβ = 0.3\n\nmix_true = MixtureModel([product_distribution(Bernoulli.(θ[:, i])) for i in 1:K], [β / 2, 1 - β, β / 2])\n\n# Generate samples from the true distribution\ny = rand(mix_true, N)\n\n# Initial Condition\nmix_guess = MixtureModel([product_distribution(Bernoulli.(2θ[:, i] / 3)) for i in 1:K], [0.25, 0.55, 0.2])\n\n# Fit MLE\nmix_mle = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"#ExpectationMaximization.jl","page":"Home","title":"ExpectationMaximization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<!– ```@contents","category":"page"},{"location":"","page":"Home","title":"Home","text":"\nThis package provides a simple implementation of the Expectation Maximization (EM) algorithm used to fit mixture models.\nDue to [Julia](https://julialang.org/) amazing [dispatch](https://www.youtube.com/watch?v=kc9HwsxE1OY) systems, generic and reusable code spirit, and the [Distributions.jl](https://juliastats.org/Distributions.jl/stable/) package, the code while being very generic is also very powerful! That means that it work on a lot of mixture:\n\n- Mixture of Univariate continuous distributions\n- Mixture of Univariate discrete distributions\n- Mixture of Multivariate distributions (continuous or discrete).\n- Mixture of mixtures (univariate or multivariate and continuous or discrete).\n\nNote that [Distributions](https://juliastats.org/Distributions.jl/stable/) *currently* does not allow `MixtureModel` to both have discrete and continuous components (but who does that? Rain).\n\n**Have a look at the [examples](@ref Examples) section**.\n\nTo work, the only requirements are that the `dist<:Distribution` considered has\n\n1. `logpdf(dist, y)` (used in the E-step)\n2. `fit_mle(dist, y, weigths)` (used in the M-step)\n\nIn general, 1. is easy, while 2. is only known explicitly for a few common distributions.\nIn case 2. is not explicit known, you can always implement a numerical scheme, if it exists, for `fit_mle(dist, y)` see [`Gamma` distribution example](https://github.com/JuliaStats/Distributions.jl/blob/34a05d8a1671052624e7fa246b58484acc32cfe5/src/univariate/continuous/gamma.jl#L171).\nOr, when possible, represent your “difficult” distribution as a mixture of simple terms.\n(I had [this](https://stats.stackexchange.com/questions/63647/estimating-parameters-of-students-t-distribution) in mind, but it is not directly a mixture model.)\n\n## Algorithms\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs ClassicEM","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs StochasticEM","category":"page"},{"location":"","page":"Home","title":"Home","text":"\n## Main function\n\n!!! warning\n    Use the \"instance\" version of `fit_mle(mix::MixtureModel, ...)` as described bellow and **NOT** the \"Type\" version i.e. `fit_mle(Type{MixtureModel}, ...)`.\n    The provided `mix` is used as the starting point of the EM algorithm.\n    See [Instance vs Type version](@ref InstanceVType) section for more context.\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs fitmle(mix::MixtureModel, y::AbstractVecOrMat, weights...; kwargs...) fitmle(mix::AbstractArray{<:MixtureModel}, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"\n## Utilities\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs mostlikelycat likelihoodpercat","category":"page"},{"location":"","page":"Home","title":"Home","text":"\n## `fit_mle` methods that should be in `Distribution.jl`\n\nI opened two PRs [PR#1670](https://github.com/JuliaStats/Distributions.jl/pull/1670) and [PR#1676](https://github.com/JuliaStats/Distributions.jl/pull/1676) to add these methods.\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs fit_mle(g::Product, x::AbstractMatrix, args...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"@docs fitmle(::Type{<:Dirac}, x::AbstractArray{T}, w::AbstractArray{Float64}) where {T<:Real} fitmle(::Type{<:Laplace}, x::AbstractArray{<:Real}, w::AbstractArray{<:Real})","category":"page"},{"location":"","page":"Home","title":"Home","text":"\n## Index\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"@index ```","category":"page"},{"location":"biblio/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"biblio/#Theory","page":"Bibliography","title":"Theory","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"The EM algorithm was introduced by A. P. Dempster, N. M. Laird and D. B. Rubin in 1977 in the reference paper Maximum Likelihood from Incomplete Data Via the EM Algorithm. This is a very generic algorithm working for almost any distributions. I also added the stochastic version introduced by G. Celeux, and J. Diebolt. in 1985 in The SEM Algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem. Other versions can be added PR are welcomed.","category":"page"},{"location":"biblio/#Implementations","page":"Bibliography","title":"Implementations","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Despite being generic, to my knowledge almost all coding implementations are specific to some mixtures class (mostly Gaussian mixtures, sometime double exponential or Bernoulli mixtures).","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In this package thanks to Julia generic code spirit one can just code the algorithm, and it works for all distributions.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I know of the Python mixem package doing also using a generic algorithm implementation. However, the available distributions choice is very limited as the authors have to define each distribution (Top-Down approach). This package does not define distribution[1], it simply uses the Distribution type and what is in Distributions.jl.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In Julia there is the GaussianMixtures.jl package that also do EM. It seems a little bit faster than my implementation when used with Gaussian mixtures (I'd like to understand what is creating this difference though). However, I am not sure if this is maintained anymore.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Have a look at the benchmark section for some comparisons.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I was inspired by Florian Oswald page and Maxime Mouchet HMMBase.jl package.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"[1]: I added fit_mle methods for Product distributions, weighted Laplace and Dirac. I am doing PR to merge that directly into the Distributions.jl package.","category":"page"}]
}
