var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I was inspired by this benchmark. I am not too sure how to do 100% fair comparisons across languages[1]. There is a small overhead for using PyCall and RCall. I checked that it was small in my experimentation (~ few milliseconds?).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I test only Gaussian Mixture since it is the most common type of mixture (remembering that this packages allow plenty of mixtures, and it should be fast in general).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"In my code, I did not use fancy programming tricks, the speed only comes from Julia, LogExpFunctions.jl for logsumexp! function and fit_mle for each distribution's coming from Distributions.jl package.","category":"page"},{"location":"benchmarks/#Univariate-Gaussian-mixture-with-2-components","page":"Benchmarks","title":"Univariate Gaussian mixture with 2 components","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I compare with Sklearn.py[2], mixtool.R, mixem.py[3]. I wanted to try mclust, but I did not manage to specify initial conditions","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Overall, mixtool.R and mixem.py were constructed in a similar spirit as this package hence easy to use for me. Sklearn.py is build to fit the Sklearn format (all in one). GaussianMixturesModel.jl is build with a similar vibe.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"If you have comments to improve these benchmarks, comments are welcomed.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"You can find the benchmark code in here.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Conclusion: for Gaussian mixture, ExpectationMaximization.jl is about 2 to 10 times faster than Python or R implementations and about as fast as the specialized Julia package GaussianMixturesModel.jl.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: timing_K_2_rudimentary_wo_memory_leak)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"<!– I guess that to increase performance in this package, it would be nice to be able to do in place fit_mle for large multidimensional cases. –>","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[1]: I would have loved that @btime with RCall and PyCall would just work.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"I did compare with R microbenchmark and Python timeit (not a pleasing experience).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[2]: This is suspect since it triggers a warning regarding K-means which I do not want to use. I asked a question here. Plus, the step by step likelihood of Sklearn is not the same as outputted by ExpectationMaximization.jl and mixtool.R (both agree), so I am a bit suspicious.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[3]: It overflows very quickly for n500 or so. I think it is because of naive implementation of logsumexp. So I eventually did not include the result in the benchmark.","category":"page"},{"location":"fit_mle/#InstanceVType","page":"Instance vs Type version","title":"Instance vs Type version","text":"","category":"section"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"This package relies heavily on the types and methods defined in Distributions.jl e.g., fit_mle and logpdf. However, it differs slightly by one point: it defines and uses an “instance version” of fit_mle(dist::Distribution, x, args...). For the package to work and be generic, the input dist must be an “instance” i.e., an actual distribution like Normal(0,1) not just the type Normal. For MixtureModels (and ProductDistributions) this is critical to extract the various distributions inside, while the Type version does not (currently) contain such information. Plus, for MixtureModels the dist serves as the initial point of the EM algorithm. For classic distributions, it changes nothing, i.e., fit_mle(Normal(0,1), y) gives the same result as fit_mle(Normal, y).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I opened a PR#1670 in Distributions.jl to include this instance version, but it might not be accepted.","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"note: Compatibility\nThe new fit_mle methods defined in this package are fully compatible with Distributions.jl (it does not break any regular Distributions.fit_mle).","category":"page"},{"location":"fit_mle/","page":"Instance vs Type version","title":"Instance vs Type version","text":"I believe that some packages dealing with complex distributions like Copulas.jl or HMMBase.jl could also use this formulation.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributions\nusing ExpectationMaximization\nusing StatsPlots","category":"page"},{"location":"examples/#Univariate-continuous","page":"Examples","title":"Univariate continuous","text":"","category":"section"},{"location":"examples/#Normal-Laplace-Exponential","page":"Examples","title":"Normal + Laplace + Exponential","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Parameters\nN = 5_000\nθ₁ = 5\nθ₂ = 0.8\nα = 0.5\nβ = 0.3\nμ = -1\na = 2\n\nmix_true = MixtureModel([Laplace(μ, θ₁), Normal(α, θ₂), Exponential(a)], [β/2, 1 - β, β/2])\n\n# Components of the mixtures\nplot(mix_true, label = [\"Laplace\" \"Normal\" \"Exponential\"])\nylabel!(\"Log PDF\", yaxis = :log10)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Sampling\ny = rand(mix_true, N)\n\n# Initial Condition\nmix_guess = MixtureModel([Laplace(-1, 1), Normal(2, 1), Exponential(3)], [1/3, 1/3, 1/3])\n\n# Fit Classic EM\nmix_mle_C, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = ClassicEM())\n\n# Fit Stochastic EM\nmix_mle_S, hist_S = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true, method = StochasticEM())\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle_C, x), label = \"fit EM\")\nplot!(pmix, x, pdf.(mix_mle_S, x), label = \"fit sEM\")\n\nploss = plot(hist_C[\"logtots\"], label = \"ClassicEM with $(hist_C[\"iterations\"]) iterations\", c = 3, xlabel = \"EM iterations\", ylabel = \"Log Likelihood\")\nplot!(ploss, hist_S[\"logtots\"], label = \"StochasticEM  with $(hist_S[\"iterations\"]) iterations\", c = 4, s = :dot)\n\nplot(pmix, ploss)","category":"page"},{"location":"examples/#Mixture-of-Mixture-and-univariate","page":"Examples","title":"Mixture of Mixture and univariate","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = -5\nθ₂ = 2\nσ₁ = 1\nσ₂ = 1.5\nθ₀ = 0.1\nσ₀ = 0.1\n\nα = 1 / 2\nβ = 0.3\n\nd1 = MixtureModel([Laplace(θ₁, σ₁), Normal(θ₂, σ₂)], [α, 1 - α])\nd2 = Normal(θ₀, σ₀)\nmix_true = MixtureModel([d1, d2], [β, 1 - β])\ny = rand(mix_true, N)\n\n# We choose initial guess very close to the true solution just to show the EM algorithm convergence.\n# This particular choice of mixture of mixture Gaussian with another Gaussian is non identifiable hence we execpt other solution far away from the true solution\nd1_guess = MixtureModel([Laplace(θ₁ - 2, σ₁ + 1), Normal(θ₂ + 1, σ₂ + 1)], [α + 0.1, 1 - α - 0.1])\nd2_guess = Normal(θ₀ - 1, σ₀ - 0.05)\n\nmix_guess = MixtureModel([d1_guess, d2_guess], [β + 0.1, 1 - β - 0.1])\nmix_mle, hist_C = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=true)\ny_guess = rand(mix_mle, N)\n\nx = -20:0.1:20\npmix = plot(x, pdf.(mix_true, x), label = \"True\", ylabel = \"PDF\")\nplot!(pmix, x, pdf.(mix_guess, x), label = \"Initial guess\")\nplot!(pmix, x, pdf.(mix_mle, x), label = \"fit EM with $(hist_C[\"iterations\"]) iterations\")","category":"page"},{"location":"examples/#Multivariate-mixtures","page":"Examples","title":"Multivariate mixtures","text":"","category":"section"},{"location":"examples/#Old-Faithful-Geyser-Data-(Multivariate-Normal)","page":"Examples","title":"Old Faithful Geyser Data (Multivariate Normal)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This seems like a canonical example for Gaussian mixtures, so let's do it. Note the use of the amazing ClipData.jl.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Using Clustering.jl package, one could easily initilize the mix_guess using K-means algorithms (and others).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using ClipData, DataFrames, StatsPlots\nusing Distributions, ExpectationMaximization\n# https://gist.githubusercontent.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv\ndata = cliptable() |> DataFrame\n\n@df data scatter(:eruptions, :waiting, label = \"Observations\", xlabel = \"Duration of the eruption (min)\", ylabel = \" Duration until the next eruption (min)\")\n\ny = permutedims(Matrix(data))\n\nD₁guess = MvNormal([22, 55], [1 0.6; 0.6 1])\nD₂guess = MvNormal([4, 80], [1 0.2; 0.2 1])\nmix_guess = MixtureModel([D₁guess, D₂guess], [1/2,1/2])\n\nmix_mle, info = fit_mle(mix_guess, y, infos = true)\n\n# mix_mleS, infoS = fit_mle(mix_guess, y, infos = true, method = StochasticEM())\n\nxrange = 1:0.05:6\nyrange = 40:0.1:100\nzlevel = [pdf(mix_mle, [x, y]) for y in yrange, x in xrange]\ncontour!(xrange, yrange, zlevel)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: old_faithful)","category":"page"},{"location":"examples/#Another-Multivariate-Gaussian-Mixtures","page":"Examples","title":"Another Multivariate Gaussian Mixtures","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"θ₁ = [-1, 1]\nθ₂ = [0, 2]\nΣ₁ = [0.5 0.5\n    0.5 1]\nΣ₂ = [1 0.1\n    0.1 1]\nβ = 0.3\nD₁ = MvNormal(θ₁, Σ₁)\nD₂ = MvNormal(θ₂, Σ₂)\n\nmix_true = MixtureModel([D₁, D₂], [β, 1 - β])\n\n# Generate samples from the true distribution\ny = rand(mix_true, N)\n\n# Initial Condition\nD₁guess = MvNormal([0.2, 1], [1 0.6; 0.6 1])\nD₂guess = MvNormal([1, 0.5], [1 0.2; 0.2 1])\nmix_guess = MixtureModel([D₁guess, D₂guess], [0.4, 0.6])\n\n# Fit MLE\nmix_mle = fit_mle(mix_guess, y; display=:none, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"examples/#MNIST-dataset:-Bernoulli-Mixture","page":"Examples","title":"MNIST dataset: Bernoulli Mixture","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"A classical example in clustering (pattern recognition) is the MNIST handwritten digits' data sets. One of the simplest ways to address the problem is to fit a Bernoulli mixture with 10 components for the ten digits 0, 1, 2, ..., 9 (see Pattern Recognition and Machine Learning by C. Bishop, Section 9.3.3. for more context). Each of the components is a product distribution of 28times 28 independent Bernoulli. This simple (but rather big) model can be fitted via the EM algorithm.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using MLDatasets: MNIST\nusing Distributions, ExpectationMaximization\nusing Plots\n\nbinarify(x) = x != 0 ? true : false\n\ndataset = MNIST(:train)\nX, y = dataset[:]\nXb = binarify.(reshape(X, (28^2, size(X, 3))))\nid = [findall(y .∈ i) for i in 0:9]\n\n# Very Informed guess (it is not true clustering since I use the label for the initial condition (IC). It also works good with other not too far IC )\ndist_guess = [product_distribution(Bernoulli.(mean(Xb[:,l] for l in id[i]))) for i in eachindex(id)]\nα = fill(1/10, 10)\n\nmix_guess = MixtureModel(dist_guess, α)\npguess = [heatmap(reshape(succprob.(dist_guess[i].v), 28,28)', yflip = :true, cmap = :grays, clims = (0,1), ticks = :none) for i in eachindex(id)]\nplot(pguess..., layout = (2,5), size = (900,300))\n\n@time mix_mle, info = fit_mle(mix_guess, Xb, infos = true, display = :iter, robust = true)\n\n# Plot the fitted mixture components\npmle = [heatmap(reshape(succprob.(components(mix_mle)[i].v), 28,28)', yflip = :true, cmap = :grays, clims = (0,1), ticks = :none) for i in eachindex(id)]\nplot(pmle..., layout = (2,5), size = (900,300))\n\n# Test results\ntest_data = MNIST(:test)\ntest_X, test_y = test_data[:]\ntest_Xb = binarify.(reshape(test_X, (28^2,size(test_X, 3))))\n\npredict_y = predict(mix_mle, test_Xb, robust = true)\n\nprintln(\"There are 28^2*10 + 9 = \", 28^2*10 + (10-1), \" parameters in the model.\")\nprintln(\"Learning accuracy \", count(predict_y.-1 .== test_y)/length(test_y), \"%.\")","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"There are 28^2*10 + 9 = 7849 parameters in the model.\n\nLearning accuracy 0.6488%.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The accuracy is of course far from the current best models (though it has a relative number of parameters). For example, this model assumes conditional independence of each pixel given the components (which is far from being true) + I am not sure the EM found the global maxima (and not just a local one).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: fit_mle_Bernoulli_mixtures)","category":"page"},{"location":"#ExpectationMaximization.jl","page":"Home","title":"ExpectationMaximization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a simple implementation of the Expectation Maximization (EM) algorithm used to fit mixture models. Due to Julia amazing dispatch systems, generic and reusable code spirit, and the Distributions.jl package, the code while being very generic is both very expressive and fast[1]! In particular, it works on a lot of mixtures:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Mixture of Univariate continuous distributions\nMixture of Univariate discrete distributions\nMixture of Multivariate distributions (continuous or discrete)\nMixture of mixtures (univariate or multivariate and continuous or discrete)\nMore?","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that Distributions currently does not allow MixtureModel to both have discrete and continuous components (but what does that? Rain).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Just define a mix::MixtureModel and do fit_mle(mix, y) with your data y and that's it! Have a look at the examples section.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To work, the only requirements are that the components of the mixture dist ∈ dists = components(mix) considered (custom or coming from an existing package)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Are a subtype of Distribution i.e. dist<:Distribution.\nThe logpdf(dist, y) is defined (it is used in the E-step)\nThe fit_mle(dist, y, weigths) returns the distribution with parameters equals to MLE. This is used in the M-step of the ClassicalEM algorithm. For the StocasticEM version, only fit_mle(dist, y) is needed. Type or instance version of fit_mle for your dist are accepted thanks to this conversion line.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general, 2. is easy, while 3. is only known explicitly for a few common distributions. In case 2. is not explicit known, you can always implement a numerical scheme, if it exists, for fit_mle(dist, y) see Gamma distribution example. Or, when possible, represent your “difficult” distribution as a mixture of simple terms. (I had this in mind, but it is not directly a mixture model.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"[1]: Have a look at the Benchmark section.","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ClassicEM","category":"page"},{"location":"#ExpectationMaximization.ClassicEM","page":"Home","title":"ExpectationMaximization.ClassicEM","text":"ClassicEM<:AbstractEM\n\nThe EM algorithm was introduced by A. P. Dempster, N. M. Laird and D. B. Rubin in 1977 in the reference paper Maximum Likelihood from Incomplete Data Via the EM Algorithm.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"StochasticEM","category":"page"},{"location":"#ExpectationMaximization.StochasticEM","page":"Home","title":"ExpectationMaximization.StochasticEM","text":"Base.@kwdef struct StochasticEM<:AbstractEM \n    rng::AbstractRNG = Random.GLOBAL_RNG\nend\n\nThe Stochastic EM algorithm was introduced by G. Celeux, and J. Diebolt. in 1985 in The SEM Algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem.\n\nThe default random seed is Random.GLOBAL_RNG but it can be changed via StochasticEM(seed).\n\n\n\n\n\n","category":"type"},{"location":"#Main-function","page":"Home","title":"Main function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"warning: Warning\nTo fit the mixture, use the “instance” version of fit_mle(mix::MixtureModel, ...) as described below and NOT the “Type” version, i.e., fit_mle(Type{MixtureModel}, ...). The provided mix is used as the starting point of the EM algorithm. See Instance vs Type version section for more context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fit_mle(mix::MixtureModel, y::AbstractVecOrMat, weights...; kwargs...)\nfit_mle(mix::AbstractArray{<:MixtureModel}, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)","category":"page"},{"location":"#Distributions.fit_mle-Tuple{MixtureModel, AbstractVecOrMat, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(mix::MixtureModel, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)\n\nUse the an Expectation Maximization (EM) algorithm to maximize the Loglikelihood (fit) the mixture with an i.i.d sample y. The mix input is a mixture that is used to initilize the EM algorithm.\n\nweights when provided, it will compute a weighted version of the EM. (Useful for fitting mixture of mixtures)\nmethod determines the algorithm used.\ninfos = true returns a Dict with informations on the algorithm (converged, iteration number, loglikelihood).\nrobust = true will prevent the (log)likelihood to overflow to -∞ or ∞.\natol criteria determining the convergence of the algorithm. If the Loglikelihood difference between two iteration i and i+1 is smaller than atol i.e. |ℓ⁽ⁱ⁺¹⁾ - ℓ⁽ⁱ⁾|<atol, the algorithm stops. \ndisplay value can be :none, :iter, :final to display Loglikelihood evolution at each iterations :iter or just the final one :final\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.fit_mle-Tuple{AbstractArray{<:MixtureModel}, AbstractVecOrMat, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(mix::AbstractArray{<:MixtureModel}, y::AbstractVecOrMat, weights...; method = ClassicEM(), display=:none, maxiter=1000, atol=1e-3, robust=false, infos=false)\n\nDo the same as fit_mle for each (initial) mixtures in the mix array. Then it selects the one with the largest loglikelihood. Warning: It uses try and catch to avoid errors messages in case EM converges toward a singular solution (probably using robust should be enough in most case to avoid errors). \n\n\n\n\n\n","category":"method"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"predict\npredict_proba","category":"page"},{"location":"#ExpectationMaximization.predict","page":"Home","title":"ExpectationMaximization.predict","text":"predict(mix::MixtureModel, y::AbstractVector; robust=false)\n\nEvaluate the most likely category for each observations given a MixtureModel.\n\nrobust = true will prevent the (log)likelihood to overflow to -∞ or ∞.\n\n\n\n\n\n","category":"function"},{"location":"#ExpectationMaximization.predict_proba","page":"Home","title":"ExpectationMaximization.predict_proba","text":"predict_proba(mix::MixtureModel, y::AbstractVecOrMat; robust=false)\n\nEvaluate the probability for each observations to belong to a category given a MixtureModel..\n\nrobust = true will prevent the (log)likelihood to under(overflow)flow to -∞ (or ∞).\n\n\n\n\n\n","category":"function"},{"location":"#fit_mle-methods-that-should-be-in-Distribution.jl","page":"Home","title":"fit_mle methods that should be in Distribution.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"I opened two PRs, PR#1670 and PR#1676 to add these methods.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fit_mle(g::Product, x::AbstractMatrix, args...)","category":"page"},{"location":"#Distributions.fit_mle-Tuple{Product, AbstractMatrix, Vararg{Any}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(g::Product, x::AbstractMatrix)\nfit_mle(g::Product, x::AbstractMatrix, γ::AbstractVector)\n\nThe fit_mle for multivariate Product distributions g is the product_distribution of fit_mle of each components of g. Product is meant to be depreacated in next version of Distribution.jl. Use the analog VectorOfUnivariateDistribution type instead.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"fit_mle(::Type{<:Dirac}, x::AbstractArray{T}, w::AbstractArray{Float64}) where {T<:Real}\nfit_mle(::Type{<:Laplace}, x::AbstractArray{<:Real}, w::AbstractArray{<:Real})","category":"page"},{"location":"#Distributions.fit_mle-Union{Tuple{T}, Tuple{Type{<:Dirac}, AbstractArray{T}, AbstractArray{Float64}}} where T<:Real","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(::Type{<:Dirac}, x::AbstractArray{<:Real}[, w::AbstractArray{<:Real}])\n\nfit_mle for Dirac distribution (weighted or not) data sets.\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.fit_mle-Tuple{Type{<:Laplace}, AbstractArray{<:Real}, AbstractArray{<:Real}}","page":"Home","title":"Distributions.fit_mle","text":"fit_mle(::Type{<:Laplace}, x::AbstractArray{<:Real}, w::AbstractArray{<:Real})\n\nfit_mle for Laplace distribution weighted data sets.\n\n\n\n\n\n","category":"method"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"biblio/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"biblio/#Theory","page":"Bibliography","title":"Theory","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"The EM algorithm was introduced by A. P. Dempster, N. M. Laird and D. B. Rubin in 1977 in the reference paper Maximum Likelihood from Incomplete Data Via the EM Algorithm. This is a very generic algorithm, working for almost any distributions. I also added the stochastic version introduced by G. Celeux, and J. Diebolt. in 1985 in The SEM Algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem. Other versions can be added PR are welcomed.","category":"page"},{"location":"biblio/#Implementations","page":"Bibliography","title":"Implementations","text":"","category":"section"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Despite being generic, to my knowledge, almost all coding implementations are specific to some mixtures class (mostly Gaussian mixtures, sometime double exponential or Bernoulli mixtures).","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In this package, thanks to Julia generic code spirit, one can just code the algorithm, and it works for all distributions.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I know of the Python mixem package doing also using a generic algorithm implementation. However, the available distribution choice is very limited as the authors have to define each distribution (Top-Down approach). This package does not define distribution[1], it simply uses the Distribution type and what is in Distributions.jl.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"In Julia, there is the GaussianMixtures.jl package that also does EM. It seems a little faster than my implementation when used with Gaussian mixtures (I'd like to understand what is creating this difference, though, maybe the in-place allocation while fit_mle creates copy). However, I am not sure if this is maintained anymore.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"Have a look at the benchmark section for some comparisons.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"I was inspired by Florian Oswald page and Maxime Mouchet HMMBase.jl package.","category":"page"},{"location":"biblio/","page":"Bibliography","title":"Bibliography","text":"[1]: I added fit_mle methods for Product distributions, weighted Laplace and Dirac. I am doing PR to merge that directly into the Distributions.jl package.","category":"page"}]
}
